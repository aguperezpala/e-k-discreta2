Herramientas empleadas:
* Empleamos la instrucción RDTSC, específica de los micros de Intel, que nos permite leer el registro "time-stamp counter". Se trata de un registro de 64 bits que contabiliza los ciclos de CPU desde que se la reseteó. Para poder usar la instrucción en nuestro programa, en lenguaje "C", hicimos uso de la función "asm", que nos permite insertar instrucciones en "assembly". La instrucción RDTSC carga los 32 bits de mayor orden del "time-stamp counter" en el registro "d", y los 32 bits de menor orden en el registro "a" (específicos de los micros Pentium). Sería posible emplear solamente los 32 bits de menor peso del registro para contabilizar los ciclos de CPU consumidos por nuestro programa, pero corremos peligro de desborde si el computo consume demasiado tiempo, teniendo en cuenta también la velocidad actual de los micros. Usando los 64 bits del "time-stamp counter", en un micro con una frecuencia de 1Ghz, tomaría aproximadamente 585 años en desbordarse (confiamos en que nuestro programa va a terminar antes...).
	La instrucción nos devuelve siempre la cantidad exacta de ciclos de CPU. De todos modos, para una medición más precisa se deben descartar los ciclos de máquinas que se emplean para realizar la medición: los ciclos que consume la instrucción RDTSC, los ciclos empleados para recomponer el resultado de la instrucción anterior en una solo valor de 64 bits, etc. Otro factor que puede influir en la medición realizada con RDTSC es la ejecución "out-of-order" de los microprocesadores Pentium, con lo cual las instrucciones no se ejecutan necesariamente en el orden en el que aparecen en el código. Luego, como RDTSC no es una instrucción "serializante" no es seguro que las instrucciones que la preceden (en el código) efectivamente se ejecuten antes que esta. Entonces, instrucciones que debieron ser medidas caen fuera de la medición . 
	Todos lo anterior motiva las siguientes decisiones de implementación:
							* Antes de cada instrucción RDTSC ejecutamos la instrucción CPUID. Se trata de una instrucción "serializante": obliga a que toda instrucción que la precede en el código se complete antes de continuar con el programa.
	                        			* Para evitar la sobrecarga de una llamada a función y terminar sumando ciclos de CPU, declaramos como "inline" a la rutina que ejecuta efectivamente a CPUID y RDTSC. De este modo, en el binario resultante al compilar el programa, no hay en realidad una llamada a la función. En su lugar está el código mismo de la función.
							* A la medición efectuada le debemos restar los ciclos invertidos en operaciones que no pertenecen al código que efectivamente queremos testear. Los ciclos invertidos en ejecutar 2 instrucciones RDTSC, 2 instrucciones CPUID y 2 "left-shifting", junto con un "or" bit a bit son los que restamos a la cuenta final.
	En este punto tuvimos problemas: al intentar obtener una estimación de los ciclos que se emplean para las operaciones de medición observamos que los resultados presentaban anomalías. Una instrucción podía consumir tantos ciclos de CPU como 2 instrucciones juntas (siendo que una era la misma instrucción anterior), o un grupo de instrucciones podía consumir menos ciclos que un grupo mayor de instrucciones (siendo las mismas instrucciones en ambos grupos). Particularmente por esto último, un promedio de los ciclos estimados no era una medida representativa. Al realizar las restas necesarias para estimar en limpio lo que consumía cada instrucción, ocurrían desbordes y al estar trabajando con valores numéricos sin signo, ese desborde resultaba en números muy grandes, los que afectarían claramente al promedio. Optamos por calcular una moda de las mediciones realizas. A nuestro favor, las experiencias nos mostraron que los desbordes no son muy frecuentes (una proporción del 0.1).
	Entre las posibles causas de estas anomalías quizás este el comportamiento de CPUID: en algunos procesadores la instrucción se ejecuta más lentamente las primeras veces mientras que en las restantes se ejecuta más rápidamente. A veces se recomienda ejecutar CPUID 3 veces seguidas para que no afecte el resultado de la medición. Tampoco tenemos un conocimiento profundo de como trabaja el compilador GCC con las funciones "asm" (por ejemplo, si interpreta que el resultado de una instrucción "assembly" no es utilizado, el compilador omite esa línea y son esta clase de optimizaciones del compilador las que podrían provocar parte de los resultados obtenidos).
	Como conclusión, reconocemos que las mediciones que hace el programa de los ciclos empleados de CPU no son confiables.
  
* Empleamos la función clock() de la libC para obtener el tiempo de CPU consumido por nuestro programa. Básicamente tomamos los ciclos consumidos al comienzo del programa, efectuamos el cómputo, y al final volvemos a ejecutar clock(). Restamos los valores y, al largo obtenido del intervalo, lo dividimos por CLOCKS_PER_SEC: el número de ticks (o pulsos de reloj) por segundo que mide la función clock(). Es decir, su precisión.

Estimación de tiempo:
	* Hicimos algunas pruebas con el programa. Trabajamos sobre una computadora con micro AMD Athlon 64, con una frecuencia de 2 Ghz, con un sistema Linux, kernel 2.6.27. Empleamos en las pruebas un network de 5000 nodos. 
	La primer prueba consistió en correr el algoritmo para construir un flujo maximal 200 veces sobre el network. Al ser tan grande el network no es necesario repetir la prueba más veces. En promedio se necesitaron 0.463500 segundos. La moda de los ciclos de CPU empleados fue de 1146728609, lo cual es razonable, teniendo en cuenta la frecuencia del micro y los ciclos que se contabilizaron que no fueron empleados efectivamente por nuestro programa (en un sistema Linux típico un proceso puede disponer de a lo sumo 0.4 segundos de procesador antes de ser intercambiado por otro proceso, bajo planificación tradicional), a diferencia de clock() que solamente devuelve los ciclos de CPU empleados por nuestro proceso. De todos modos, para hacer un estudio más preciso de esto último habría que conocer a fondo los mecanismos de planificación del sistema.  
	Sobre el mismo network probamos los algoritmos de coloreo. Repetimos la prueba 200 veces también. Se pensó en medir la diferencia de ejecución entre la primer llamada a ColorearNetwork (momento en que se ejecuta el algoritmo "first-coloring") y las restantes, cuando se emplea Greedy. Para cargar el network y la primer llamada a ColorearNetwork se necesitaron  0.010000 en promedio. Para las restantes llamadas se necesitaron 0.004500, en promedio también. En principio "first-coloring" es más eficiente que la implementación del algoritmo "greedy", pero al cargarse el network al momento de llamar a first-coloring se pierde en tiempo. La moda de los ciclos de CPU empleados fue de 23664576 y 9822011 para "first-coloring" y "greedy", respectivamente. Nuevamente aparecen las pequeñas discrepancias entre el tiempo empleado y los ciclos de CPU. Esto también se explica en función de ciclos de CPU contabilizados que no pertenecen a nuestro proceso, mientras que clock() solo devuelve ciclos de CPU empleados por nuestro proceso, a una resolución de CLOCKS_PER_SEC ticks por segundo.
	Recalcamos las dificultades que se pueden llegar a tener al medir ciclos de CPU. Si las pruebas se llevaran a cabo sobre un procesador con 2 o más núcleos, dependiendo de los flags con los que haya sido compilado el kernel, el resultado puede ser totalmente incorrecto.
 
